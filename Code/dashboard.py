import streamlit as st
import pandas as pd
import numpy as np
import torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer

from vector_search import cosine_retrieve

# ======================
# CONFIG
# ======================
NLI_MODEL_PATH = "Model"
EMBED_MODEL = "AITeamVN/Vietnamese_Embedding"

EMB_PATH = "Dataset/premise_embeddings.npy"
META_PATH = "Dataset/premise_meta.parquet"

THRESHOLD = 0.75
TOP_K_RETRIEVE = 30
TOP_K_SHOW = 5

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

st.set_page_config(page_title="Legal Entailment Demo", layout="wide")

# ======================
# LOAD MODELS (CACHE)
# ======================
@st.cache_resource
def load_models():
    embedder = SentenceTransformer(EMBED_MODEL, device=DEVICE)

    tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)
    nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH)
    nli_model.to(DEVICE)
    nli_model.eval()

    return embedder, tokenizer, nli_model


@st.cache_data
def load_data():
    emb = np.load(EMB_PATH)
    meta = pd.read_parquet(META_PATH)
    return emb, meta


embedder, tokenizer, nli_model = load_models()
premise_embeddings, meta_df = load_data()

# ======================
# UI
# ======================
st.title("âš–ï¸ Há»† THá»NG TRUY XUáº¤T VÃ€ SUY LUáº¬N PHÃP LÃ VIá»†T NAM")

hypothesis = st.text_area(
    "Nháº­p giáº£ thuyáº¿t phÃ¡p lÃ½:",
    height=120
)

run_btn = st.button("ðŸ” Kiá»ƒm tra cÄƒn cá»© phÃ¡p lÃ½")

# ======================
# NLI
# ======================
def batch_nli(premises, hypothesis):
    hypotheses = [hypothesis] * len(premises)

    inputs = tokenizer(
        premises,
        hypotheses,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=256
    ).to(DEVICE)

    with torch.no_grad():
        logits = nli_model(**inputs).logits

    probs = torch.softmax(logits, dim=1)

    # BÃ i toÃ¡n 2 nhÃ£n: ENTAILMENT vs NON-ENTAILMENT
    # Giáº£ Ä‘á»‹nh model: [NON-ENTAILMENT, ENTAILMENT]
    return probs[:, 1].cpu().numpy()


def render_legal_citation(row):
    parts = []

    if pd.notna(row.get("Tag Point")):
        parts.append(f"Äiá»ƒm {row['Tag Point']}")
    if pd.notna(row.get("Clause")):
        parts.append(f"Khoáº£n {row['Clause']}")
    if pd.notna(row.get("Article")):
        parts.append(f"Äiá»u {row['Article']}")

    citation = ", ".join(parts)

    law_info = []
    if pd.notna(row.get("Law Name")):
        law_info.append(row["Law Name"])
    if pd.notna(row.get("Law ID")):
        law_info.append(f"Sá»‘: {row['Law ID']}")
    if pd.notna(row.get("Law Date")):
        law_info.append(f"({row['Law Date']})")

    return citation, " ".join(law_info)


# ======================
# RUN
# ======================
if run_btn and hypothesis.strip():
    with st.spinner("â³ Äang suy luáº­n phÃ¡p lÃ½..."):
        # 1ï¸âƒ£ Embed hypothesis
        query_emb = embedder.encode(
            hypothesis,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # 2ï¸âƒ£ Retrieve top-K premise
        idxs, sim_scores = cosine_retrieve(
            query_emb,
            premise_embeddings,
            top_k=TOP_K_RETRIEVE
        )

        candidates = meta_df.iloc[idxs].copy()
        candidates["similarity"] = sim_scores

        # 3ï¸âƒ£ NLI trÃªn tá»«ng premise
        entail_probs = batch_nli(
            candidates["Premise"].tolist(),
            hypothesis
        )
        candidates["entail_prob"] = entail_probs

        candidates = candidates.sort_values(
            "entail_prob", ascending=False
        )

        # 4ï¸âƒ£ AGGREGATION â†’ SUY LUáº¬N CUá»I
        # dÃ¹ng top-3 premise máº¡nh nháº¥t
        final_score = np.max(candidates.head(3)["entail_prob"])

        final_label = (
            "ENTAILMENT" if final_score >= THRESHOLD
            else "NON-ENTAILMENT"
        )

    st.divider()

    # ======================
    # RESULT
    # ======================
    if final_label == "ENTAILMENT":
        st.success("âœ… **CÃ‚U PHÃT BIá»‚U TRÃŠN LÃ€ ÄÃšNG**")
        st.caption(f"Äiá»ƒm suy luáº­n cuá»‘i: **{final_score:.3f}**")

        st.subheader("ðŸ“Œ Nháº­p cÄƒn cá»© phÃ¡p lÃ½ há»— trá»£")
        for _, r in candidates.head(TOP_K_SHOW).iterrows():
            citation, law_info = render_legal_citation(r)

            st.markdown(f"""
**Entailment:** `{r.entail_prob:.3f}`  
**Similarity:** `{r.similarity:.3f}`  

ðŸ“Œ **CÄƒn cá»© phÃ¡p lÃ½:**  
{citation}  

ðŸ“˜ **VÄƒn báº£n:**  
{law_info}  

> {r.Premise}
""")
            st.markdown("---")

    else:
        st.error("âŒ **CÃ‚U PHÃT BIá»‚U TRÃŠN KHÃ”NG ÄÃšNG**")
        st.caption(f"Äiá»ƒm suy luáº­n cao nháº¥t: **{final_score:.3f}**")

        st.subheader("ðŸ” CÃ¡c Ä‘iá»u luáº­t liÃªn quan (chÆ°a Ä‘á»§ cÄƒn cá»©)")
        for _, r in candidates.head(TOP_K_SHOW).iterrows():
            st.markdown(f"""
**Entailment:** `{r.entail_prob:.3f}`  
**Similarity:** `{r.similarity:.3f}`  

> {r.Premise}
""")
            st.markdown("---")
